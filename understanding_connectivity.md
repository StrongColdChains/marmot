# Understanding Connectivity

The connection tier is responsible for getting the data generated by
sensors into some central server for data processing. This typically
happens in two stages, one from getting the sensor data to a central
base station in the facility, and then getting the data from the base station
to the central server.

These two hops are fairly different in nature in multiple dimensions. First,
getting from the sensor to a base station is typically accomplished using some
local network protocol over the air like zigbee or bluetooth, or is simply transmitted
over a physical conneciton. Getting from the base station to the central server is
typically over a significantly larger distance, using the internet over something like
a wired connection, a sim card, or satellite.

The fundamental differences in physical mediums used to propogate the signal means that
their failure modes are very different. sensor to base station connection can be
severed if the signal is blocked or a wire is unplugged. base station to server connection
can be severed if you forget to pay your cell provider, if a power outage cuts out wifi,
things like that.

# received_at

One proprty of IoT data that makes it particularly interesting is that all data points come
tagged with two timestamps, the created_at and received_at attributes. Basically every
dashboard will focus on created_at, not all of them will attempt to mine received_at
for information. The time data was received lets you understand where connectivity is
problematic. If everything is working fine, we'd expect received_at to slightly lag
created_at, with the delta typically being the time it takes to transmit the information
over the network. If you batch your network calls (ie, even though your sensor takes
readings every second, we only send the data once a minute) then that would also cause
received_at to lag.

Once connectivity outages start to occur, we'd see the two timestamps start to diverge
more dramatically. The larger the divergance, the longer the time it took for the data
to be successfully received by the central server.


Suppose a power outage in a remote facility causes 5 CCEs to go offline
for the months of July and August, coming back online in September. In the monthly reports
for July and August, we wouldn't have any data for these CCEs. Once the monthly report
for September comes in, we'd see the CCE's data for September, as well as July and August!
Importantly, there now becomes some ambiguity when a user requests a report showing the
data for the month of July. Do they mean they want to see all of the data we *currently* have
for July? Or do they mean the data that was visible to users in July?

This ambiguity is responsible for a significant amount of friction between different
stakeholders and most of them don't have the language required to express what it is
that they want. It's important for any software to provide appropriate filters on both
created_at as well as received_at in order to fulfill user queries. It's also important to
educate users on the distinctions between these two timestamps so users know what to ask for
and how to interpret reports.

When I worked for Nexleaf this complexity caused quite a bit of friction. It wasn't uncommon
for users to print out the monthly reports they'd receive from us and put them into
filing cabinets. If a user requested data from months ago, we'd give them a new report
that incorporated stale data that had arrived late. But this would cause confusion when
they'd pull out their old reports and compare them, realizing that the numbers didn't line
up.

Different user stories require different approaches to displaying data. If the user is looking
to compile a report on CCE performance by brand, there's no issue with including late-arriving
data. However, a user looking to track the number offline CCEs per month would definitely need
to ignore late-arriving data.



# Definitions

- Data Lag: the difference between created_at and received_at.
- Lateness Threshold: The amount of time that must pass before a datapoint is considered late.
This threshold should take into account any batching that base stations perform as well as any
inherent latency in the network. We recommend calling data late once it's passed 1.25 times the
batching time. If the data isn't being batched, then try 15 minutes.


# An Example!
Putting these definitions together, suppose you have a deployment where a base station sends
its data hourly, and its sensors take readings every 10 minutes. A well-functioning base station
might look something like this.

```csv
created_at,received_at,lag,is_late,lateness_amount
2023-11-21 00:00:00+00,2023-11-21 01:00:00+00,60,false,0
2023-11-21 00:10:00+00,2023-11-21 01:00:00+00,50,false,0
2023-11-21 00:20:00+00,2023-11-21 01:00:00+00,40,false,0
2023-11-21 00:30:00+00,2023-11-21 01:00:00+00,30,false,0
2023-11-21 00:40:00+00,2023-11-21 01:00:00+00,20,false,0
2023-11-21 00:50:00+00,2023-11-21 01:00:00+00,10,false,0
2023-11-21 01:00:00+00,2023-11-21 01:00:00+00,0,false,0
```

Now let's consider a base station that experiences some network issues when it tries to send
data on the hour, and can only send the data 1 hours later:

```csv
created_at,received_at,lag,is_late,lateness_amount
2023-11-21 00:00:00+00,2023-11-21 02:00:00+00,120,true,30
2023-11-21 00:10:00+00,2023-11-21 02:00:00+00,110,true,20
2023-11-21 00:20:00+00,2023-11-21 02:00:00+00,100,true,10
2023-11-21 00:30:00+00,2023-11-21 02:00:00+00,90,false,0
2023-11-21 00:40:00+00,2023-11-21 02:00:00+00,80,false,0
2023-11-21 00:50:00+00,2023-11-21 02:00:00+00,70,false,0
2023-11-21 01:00:00+00,2023-11-21 02:00:00+00,60,false,0
```

Note that, even though all the datapoints are later, only half of them are actually
considered "late" since our lateness threshold is defined as 90 minutes (1.5 times the batching
window of 60 minutes). Keeping around both lag and lateness_amount is useful for later
data analysis.

# KPIs

- Cumulative Lateness Amount: The cumulative amount of datapoint lateness that a base station
has accrued over the last 30 days
- Average Lateness Amount: The average amount of datapoint lateness
that a base station has sent over the last 30 days.
- Largest Lateness Amount: The largest lateness amount that a base station has sent. This amounts
to the biggest connectivity outage experienced by this base station.
- Lateness Percentage: The percent of datapoints that were late in the last 30 days.


note: we probably shouldn't just show the largest lateness amount. We should to quantile information
to give better information!